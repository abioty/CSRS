
################################################################################
#  Social-Risk Composite Score Development with Tidymodels
################################################################################

# 0. Setup and Dependencies ----
library(tidymodels)
library(tidyverse)
library(stacks)
library(ranger)
library(glmnet)
library(mice)
library(permute)
library(jsonlite)
library(pls)
library(vip)
library(doParallel)
library(FactoMineR)
library(cluster)
library(factoextra)
library(broom)
library(boot)
library(patchwork)
library(knitr)
library(kableExtra)

# Use future backend
library(future)
plan(multisession, workers = max(1, availableCores() - 1))

tidymodels_prefer()
theme_set(theme_minimal(base_size = 11))

# 1. Data Loading and Initial Setup ----
cat("Loading and preparing data...\n")

df <- readr::read_csv("social_risk_factors_with_dummies1.csv", show_col_types = FALSE)

# Define original categorical variables (before reference group removal)
education_vars  <- c("Education_College degree", "Education_Graduate degree and higher",
                     "Education_High school or less")
income_vars     <- c("Income_High income (>=$80,000)", "Income_Low income (<$40,000)",
                     "Income_Middle income ($40,000-$79,999)")
employment_vars <- c("Employment_Employed", "Employment_Stay-at-home caregiver",
                     "Employment_Student", "Employment_Unemployed")
family_vars     <- c("Family_Cohabiting", "Family_Married", "Family_Single")

# Define reference groups (most protective categories)
reference_groups <- list(
  education = "Education_Graduate degree and higher",
  income = "Income_High income (>=$80,000)",
  employment = "Employment_Employed",
  family = "Family_Married"
)

# Create maternal age categories
df <- df %>%
  mutate(
    mat_age_young = as.numeric(mat_age <= 25),  # 1 if young (risk), 0 if older (reference)
    mat_age_cat = factor(ifelse(mat_age <= 25, "Young (≤25)", "Older (>25)"),
                         levels = c("Older (>25)", "Young (≤25)"))  # Older as reference
  )

# Define predictors EXCLUDING reference categories
# Remove reference categories from each group
education_predictors <- setdiff(education_vars, reference_groups$education)
income_predictors <- setdiff(income_vars, reference_groups$income)
employment_predictors <- setdiff(employment_vars, reference_groups$employment)
family_predictors <- setdiff(family_vars, reference_groups$family)

# Combine all predictors including mat_age_young
predictors <- c("mat_age_young", education_predictors, income_predictors, 
                employment_predictors, family_predictors)

# Define outcomes
outcomes <- c("cog_comp_score", "lang_comp_score", "motor_comp_score")
primary_outcome <- "cog_comp_score"

# Create analysis dataset
df_analysis <- df %>% 
  dplyr::select(study_id, mat_age, dplyr::all_of(predictors), dplyr::all_of(outcomes))

cat("Dataset summary:\n")
cat("- Total observations:", nrow(df_analysis), "\n")
cat("- Number of predictors:", length(predictors), "\n")
cat("- Number of outcomes:", length(outcomes), "\n")
cat("- PRIMARY OUTCOME:", primary_outcome, "\n")
cat("- Missing outcome data:", sum(!complete.cases(df_analysis[outcomes])), "rows\n")
cat("\nReference groups (excluded from models):\n")
cat("- Education:", reference_groups$education, "\n")
cat("- Income:", reference_groups$income, "\n")
cat("- Employment:", reference_groups$employment, "\n")
cat("- Family:", reference_groups$family, "\n")
cat("- Maternal age: Older (>25) [reference]\n\n")

# 2. Multiple Imputation for Outcomes ----
cat("Performing multiple imputation for outcomes...\n")

impute_data <- df_analysis %>% 
  dplyr::select(dplyr::all_of(predictors), dplyr::all_of(outcomes))

mice_config  <- mice(impute_data, m = 5, method = "pmm", printFlag = FALSE, seed = 2025)
imputed_list <- complete(mice_config, action = "all")

df_imputed <- dplyr::bind_cols(
  df_analysis %>% dplyr::select(study_id, mat_age),  # Keep original mat_age for reference
  imputed_list[[1]]
)

cat("Imputation complete. Working with", nrow(df_imputed), "observations.\n\n")

# 3. Create Unsupervised Features ----
cat("Creating unsupervised features with maternal age categories...\n")

# 3.1 Prepare data for FAMD
famd_data <- df_imputed %>%
  select(all_of(predictors))

# Convert to factors for FAMD
famd_clean <- data.frame(
  mat_age_cat = factor(ifelse(df_imputed$mat_age_young == 1, "Young (≤25)", "Older (>25)"),
                       levels = c("Young (≤25)", "Older (>25)")),
  Education = apply(df_imputed[, education_predictors], 1, function(x) {
    if(sum(x) == 0) return(reference_groups$education)
    education_predictors[which(x == 1)]
  }),
  Income = apply(df_imputed[, income_predictors], 1, function(x) {
    if(sum(x) == 0) return(reference_groups$income)
    income_predictors[which(x == 1)]
  }),
  Employment = apply(df_imputed[, employment_predictors], 1, function(x) {
    if(sum(x) == 0) return(reference_groups$employment)
    employment_predictors[which(x == 1)]
  }),
  Family = apply(df_imputed[, family_predictors], 1, function(x) {
    if(sum(x) == 0) return(reference_groups$family)
    family_predictors[which(x == 1)]
  })
)

# Clean factor names
for(col in c("Education", "Income", "Employment", "Family")) {
  famd_clean[[col]] <- gsub(".*_", "", famd_clean[[col]])
  famd_clean[[col]] <- factor(famd_clean[[col]])
}

# Run FAMD
famd_result <- FAMD(famd_clean, graph = FALSE, ncp = 5)
famd_dim1 <- famd_result$ind$coord[, 1]

cat("FAMD: First dimension explains", 
    round(famd_result$eig[1, 2], 1), "% of variance\n")

# 3.2 MCA for categorical variables
mca_result <- MCA(famd_clean, graph = FALSE, ncp = 5)
mca_dim1 <- mca_result$ind$coord[, 1]

cat("MCA: First dimension explains", 
    round(mca_result$eig[1, 2], 1), "% of variance\n")

# 3.3 PAM clustering with Gower distance
gower_dist <- cluster::daisy(famd_clean, metric = "gower")

# Find optimal number of clusters
sil_width <- numeric(4)
for(k in 2:5) {
  pam_fit <- pam(gower_dist, k = k)
  sil_width[k-1] <- pam_fit$silinfo$avg.width
}

optimal_k <- which.max(sil_width) + 1
pam_final <- pam(gower_dist, k = optimal_k)

# Order clusters by cognitive outcome
outcome_means <- aggregate(df_imputed[[primary_outcome]],
                           by = list(pam_final$clustering),
                           FUN = mean)
cluster_order <- order(outcome_means$x)
pam_clusters <- factor(pam_final$clustering,
                       levels = cluster_order,
                       labels = paste0("Cluster_", 1:optimal_k))

cat("PAM: Optimal number of clusters =", optimal_k,
    "with silhouette width =", round(max(sil_width), 3), "\n\n")

# Add unsupervised features to data
df_imputed$FAMD_Dim1 <- famd_dim1
df_imputed$MCA_Dim1 <- mca_dim1
df_imputed$PAM_Cluster <- as.numeric(pam_clusters)

# 4. Model specifications ----
cat("Defining model specifications...\n")

# Supervised models
ols_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

elastic_spec <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

rf_spec <- rand_forest(mtry = tune(), trees = 500, min_n = tune()) %>% 
  set_engine("ranger", importance = "permutation") %>% 
  set_mode("regression")

# Unsupervised models
unsup_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# 5. Function to extract weights for risk score with proper bootstrap ----
extract_model_weights <- function(data, model_name, outcome, n_boot = 1000) {
  
  cat("  Extracting weights for", model_name, "model...\n")
  
  # First fit the model on full data to get baseline weights
  X_full <- model.matrix(~ . - 1, data[predictors])
  y_full <- data[[outcome]]
  
  if (model_name == "ridge") {
    # Use minimal penalty Ridge for interpretable weights
    ridge_fit <- glmnet(X_full, y_full, alpha = 0, 
                        lambda = 0.001,
                        standardize = TRUE)
    
    full_weights <- as.numeric(coef(ridge_fit))[-1]
    names(full_weights) <- predictors
    
    cat("    Using lambda = 0.001 for interpretable risk score weights\n")
    
  } else if (model_name == "lasso") {
    cv_lasso <- cv.glmnet(X_full, y_full, alpha = 1, 
                          standardize = TRUE,
                          nfolds = 10)
    
    full_weights <- as.numeric(coef(cv_lasso, s = "lambda.min"))[-1]
    names(full_weights) <- predictors
    
  } else if (model_name == "elastic") {
    cv_elastic <- cv.glmnet(X_full, y_full, alpha = 0.5, 
                            standardize = TRUE,
                            nfolds = 10)
    
    full_weights <- as.numeric(coef(cv_elastic, s = "lambda.min"))[-1]
    names(full_weights) <- predictors
    
  } else if (model_name == "ols") {
    ols_fit <- lm(as.formula(paste(outcome, "~ .")), 
                  data = data[c(predictors, outcome)])
    full_weights <- coef(ols_fit)[-1]
    
    # Handle NA coefficients (shouldn't happen with proper reference groups)
    if(any(is.na(full_weights))) {
      cat("    WARNING: NA coefficients detected. Check for multicollinearity.\n")
      full_weights[is.na(full_weights)] <- 0
    }
    
  } else if (model_name == "rf") {
    rf_fit <- rand_forest(mtry = 5, trees = 500, min_n = 5) %>%
      set_engine("ranger", importance = "permutation") %>%
      set_mode("regression") %>%
      fit(as.formula(paste(outcome, "~ .")), 
          data = data[c(predictors, outcome)])
    
    importance <- rf_fit$fit$variable.importance
    correlations <- cor(data[predictors], data[[outcome]], use = "complete.obs")
    full_weights <- importance * sign(correlations[names(importance), 1])
  }
  
  # Bootstrap for confidence intervals
  boot_weights <- matrix(NA, nrow = n_boot, ncol = length(predictors))
  colnames(boot_weights) <- predictors
  
  successful_boots <- 0
  
  for (b in 1:n_boot) {
    boot_idx <- sample(nrow(data), replace = TRUE)
    boot_data <- data[boot_idx, ]
    
    tryCatch({
      if (model_name == "ridge") {
        X_boot <- model.matrix(~ . - 1, boot_data[predictors])
        y_boot <- boot_data[[outcome]]
        
        ridge_boot <- glmnet(X_boot, y_boot, alpha = 0, 
                             lambda = 0.001,
                             standardize = TRUE)
        
        weights <- as.numeric(coef(ridge_boot))[-1]
        
      } else if (model_name == "lasso") {
        X_boot <- model.matrix(~ . - 1, boot_data[predictors])
        y_boot <- boot_data[[outcome]]
        
        cv_boot <- cv.glmnet(X_boot, y_boot, alpha = 1, 
                             standardize = TRUE,
                             nfolds = 5)
        
        weights <- as.numeric(coef(cv_boot, s = "lambda.min"))[-1]
        
      } else if (model_name == "elastic") {
        X_boot <- model.matrix(~ . - 1, boot_data[predictors])
        y_boot <- boot_data[[outcome]]
        
        cv_boot <- cv.glmnet(X_boot, y_boot, alpha = 0.5, 
                             standardize = TRUE,
                             nfolds = 5)
        
        weights <- as.numeric(coef(cv_boot, s = "lambda.min"))[-1]
        
      } else if (model_name == "ols") {
        lm_boot <- lm(as.formula(paste(outcome, "~ .")), 
                      data = boot_data[c(predictors, outcome)])
        weights <- coef(lm_boot)[-1]
        weights[is.na(weights)] <- 0
        
      } else if (model_name == "rf") {
        rf_boot <- rand_forest(mtry = 5, trees = 500, min_n = 5) %>%
          set_engine("ranger", importance = "permutation") %>%
          set_mode("regression") %>%
          fit(as.formula(paste(outcome, "~ .")), 
              data = boot_data[c(predictors, outcome)])
        
        importance <- rf_boot$fit$variable.importance
        correlations <- cor(boot_data[predictors], boot_data[[outcome]], 
                            use = "complete.obs")
        weights <- importance * sign(correlations[names(importance), 1])
      }
      
      boot_weights[b, ] <- weights
      successful_boots <- successful_boots + 1
      
    }, error = function(e) {
      # Skip this bootstrap sample if it fails
      boot_weights[b, ] <- NA
    })
  }
  
  # Remove failed bootstrap samples
  boot_weights <- boot_weights[complete.cases(boot_weights), , drop = FALSE]
  
  # Calculate statistics from bootstrap samples
  weight_stats <- data.frame(
    Variable = names(full_weights),
    Mean = full_weights,
    SD = apply(boot_weights, 2, sd, na.rm = TRUE),
    CI_Lower = apply(boot_weights, 2, quantile, probs = 0.025, na.rm = TRUE),
    CI_Upper = apply(boot_weights, 2, quantile, probs = 0.975, na.rm = TRUE),
    stringsAsFactors = FALSE
  )
  
  # Calculate p-values based on bootstrap distribution
  weight_stats$p_value <- sapply(1:length(full_weights), function(i) {
    if (abs(full_weights[i]) < 1e-10) {
      return(1)
    }
    # Proportion of bootstrap samples with opposite sign
    if (full_weights[i] > 0) {
      p_neg <- mean(boot_weights[, i] <= 0)
    } else {
      p_neg <- mean(boot_weights[, i] >= 0)
    }
    # Two-tailed p-value
    return(2 * min(p_neg, 1 - p_neg))
  })
  
  # Add additional statistics
  weight_stats$SE <- weight_stats$SD / sqrt(nrow(boot_weights))
  weight_stats$t_stat <- ifelse(weight_stats$SE > 0, 
                                weight_stats$Mean / weight_stats$SE, 0)
  
  # Diagnostic output
  cat("    Successful bootstrap samples:", nrow(boot_weights), "\n")
  cat("    Weight range: [", round(min(weight_stats$Mean), 4), ",", 
      round(max(weight_stats$Mean), 4), "]\n")
  
  return(weight_stats)
}

# Add function for unsupervised weight extraction with proper bootstrap
extract_unsupervised_weights <- function(data, model_name, outcome, n_boot = 1000) {
  
  cat("  Extracting unsupervised weights for", model_name, "...\n")
  
  # Initialize weights vector for all predictors
  full_weights <- numeric(length(predictors))
  names(full_weights) <- predictors
  
  # Get full data weights based on model type
  if (model_name == "FAMD_Dim1") {
    # Get coordinates for qualitative variables
    if (!is.null(famd_result$quali.var) && !is.null(famd_result$quali.var$coord)) {
      famd_quali_coords <- famd_result$quali.var$coord[, 1]
      cat("    FAMD qualitative coordinates found:", length(famd_quali_coords), "variables\n")
    } else {
      cat("    ERROR: No qualitative coordinates found in FAMD result\n")
      return(data.frame(
        Variable = predictors,
        Mean = rep(0, length(predictors)),
        SD = rep(NA, length(predictors)),
        CI_Lower = rep(NA, length(predictors)),
        CI_Upper = rep(NA, length(predictors)),
        SE = rep(NA, length(predictors)),
        t_stat = rep(NA, length(predictors)),
        p_value = rep(NA, length(predictors)),
        stringsAsFactors = FALSE
      ))
    }
    
    # Get correlation with outcome for sign
    famd_cor <- cor(data$FAMD_Dim1, data[[outcome]], use = "complete.obs")
    cat("    FAMD-outcome correlation:", round(famd_cor, 3), "\n")
    
    # Map FAMD qualitative coordinates to our predictors
    for (var_name in names(famd_quali_coords)) {
      coord_value <- famd_quali_coords[var_name] * sign(famd_cor)
      
      # Direct mapping based on variable names
      if (var_name == "Young (≤25)") {
        full_weights["mat_age_young"] <- coord_value
      } else if (var_name == "College degree") {
        full_weights["Education_College degree"] <- coord_value
      } else if (var_name == "High school or less") {
        full_weights["Education_High school or less"] <- coord_value
      } else if (var_name == "Low income (<$40,000)") {
        full_weights["Income_Low income (<$40,000)"] <- coord_value
      } else if (var_name == "Middle income ($40,000-$79,999)") {
        full_weights["Income_Middle income ($40,000-$79,999)"] <- coord_value
      } else if (var_name == "Stay-at-home caregiver") {
        full_weights["Employment_Stay-at-home caregiver"] <- coord_value
      } else if (var_name == "Student") {
        full_weights["Employment_Student"] <- coord_value
      } else if (var_name == "Unemployed") {
        full_weights["Employment_Unemployed"] <- coord_value
      } else if (var_name == "Cohabiting") {
        full_weights["Family_Cohabiting"] <- coord_value
      } else if (var_name == "Single") {
        full_weights["Family_Single"] <- coord_value
      }
    }
    
  } else if (model_name == "MCA_Dim1") {
    # MCA uses coordinates from var$coord
    mca_coords <- mca_result$var$coord[, 1]
    
    cat("    MCA coordinates found:", length(mca_coords), "variables\n")
    
    mca_cor <- cor(data$MCA_Dim1, data[[outcome]], use = "complete.obs")
    cat("    MCA-outcome correlation:", round(mca_cor, 3), "\n")
    
    # Map MCA variable names to predictors
    for (var_name in names(mca_coords)) {
      coord_value <- mca_coords[var_name] * sign(mca_cor)
      
      # MCA uses format "Variable.Level"
      if (grepl("\\.", var_name)) {
        parts <- strsplit(var_name, "\\.")[[1]]
        var_type <- parts[1]
        level <- paste(parts[-1], collapse = ".")
        
        if (var_type == "mat_age_cat" && level == "Young (≤25)") {
          full_weights["mat_age_young"] <- coord_value
        } else if (var_type == "Education" && level == "College degree") {
          full_weights["Education_College degree"] <- coord_value
        } else if (var_type == "Education" && level == "High school or less") {
          full_weights["Education_High school or less"] <- coord_value
        } else if (var_type == "Income" && grepl("Low income", level)) {
          full_weights["Income_Low income (<$40,000)"] <- coord_value
        } else if (var_type == "Income" && grepl("Middle income", level)) {
          full_weights["Income_Middle income ($40,000-$79,999)"] <- coord_value
        } else if (var_type == "Employment" && grepl("Stay.*home", level)) {
          full_weights["Employment_Stay-at-home caregiver"] <- coord_value
        } else if (var_type == "Employment" && level == "Student") {
          full_weights["Employment_Student"] <- coord_value
        } else if (var_type == "Employment" && level == "Unemployed") {
          full_weights["Employment_Unemployed"] <- coord_value
        } else if (var_type == "Family" && level == "Cohabiting") {
          full_weights["Family_Cohabiting"] <- coord_value
        } else if (var_type == "Family" && level == "Single") {
          full_weights["Family_Single"] <- coord_value
        }
      }
    }
    
  } else if (model_name == "PAM_Cluster") {
    # PAM doesn't give feature weights
    cat("    Note: PAM clustering doesn't provide individual feature weights\n")
    return(data.frame(
      Variable = predictors,
      Mean = rep(0, length(predictors)),
      SD = rep(NA, length(predictors)),
      CI_Lower = rep(NA, length(predictors)),
      CI_Upper = rep(NA, length(predictors)),
      SE = rep(NA, length(predictors)),
      t_stat = rep(NA, length(predictors)),
      p_value = rep(NA, length(predictors)),
      stringsAsFactors = FALSE
    ))
  }
  
  # Debug: Print full weights
  cat("    Extracted weights summary:\n")
  non_zero_weights <- full_weights[abs(full_weights) > 0.001]
  if (length(non_zero_weights) > 0) {
    for (i in 1:min(5, length(non_zero_weights))) {
      cat("      ", names(non_zero_weights)[i], ":", round(non_zero_weights[i], 4), "\n")
    }
    if (length(non_zero_weights) > 5) {
      cat("      ... and", length(non_zero_weights) - 5, "more\n")
    }
  } else {
    cat("      WARNING: All weights are zero!\n")
  }
  
  # Bootstrap for confidence intervals - PROPER METHOD
  cat("    Running bootstrap (n=", n_boot, ")...\n", sep = "")
  boot_weights <- matrix(0, nrow = n_boot, ncol = length(predictors))
  colnames(boot_weights) <- predictors
  
  # Prepare data for bootstrap
  famd_vars <- c("mat_age_cat", "Education", "Income", "Employment", "Family")
  
  for (b in 1:n_boot) {
    if (b %% 50 == 0) cat("      Bootstrap sample", b, "\n")
    
    # Resample data
    boot_idx <- sample(nrow(data), replace = TRUE)
    boot_data <- data[boot_idx, ]
    
    if (model_name == "FAMD_Dim1") {
      # Prepare data for FAMD
      famd_boot_data <- data.frame(
        mat_age_cat = factor(ifelse(boot_data$mat_age_young == 1, "Young (≤25)", "Older (>25)"),
                             levels = c("Young (≤25)", "Older (>25)")),
        Education = apply(boot_data[, education_predictors], 1, function(x) {
          if(sum(x) == 0) return(reference_groups$education)
          education_predictors[which(x == 1)]
        }),
        Income = apply(boot_data[, income_predictors], 1, function(x) {
          if(sum(x) == 0) return(reference_groups$income)
          income_predictors[which(x == 1)]
        }),
        Employment = apply(boot_data[, employment_predictors], 1, function(x) {
          if(sum(x) == 0) return(reference_groups$employment)
          employment_predictors[which(x == 1)]
        }),
        Family = apply(boot_data[, family_predictors], 1, function(x) {
          if(sum(x) == 0) return(reference_groups$family)
          family_predictors[which(x == 1)]
        })
      )
      
      # Clean factor names
      for(col in c("Education", "Income", "Employment", "Family")) {
        famd_boot_data[[col]] <- gsub(".*_", "", famd_boot_data[[col]])
        famd_boot_data[[col]] <- factor(famd_boot_data[[col]])
      }
      
      # Run FAMD on bootstrap sample
      tryCatch({
        boot_famd <- FAMD(famd_boot_data, graph = FALSE, ncp = 1)
        boot_coords <- boot_famd$quali.var$coord[, 1]
        
        # Get correlation for sign
        boot_cor <- cor(boot_famd$ind$coord[, 1], boot_data[[outcome]], use = "complete.obs")
        
        # Extract weights for this bootstrap sample
        for (var_name in names(boot_coords)) {
          coord_value <- boot_coords[var_name] * sign(boot_cor)
          
          if (var_name == "Young (≤25)") {
            boot_weights[b, "mat_age_young"] <- coord_value
          } else if (var_name == "College degree") {
            boot_weights[b, "Education_College degree"] <- coord_value
          } else if (var_name == "High school or less") {
            boot_weights[b, "Education_High school or less"] <- coord_value
          } else if (var_name == "Low income (<$40,000)") {
            boot_weights[b, "Income_Low income (<$40,000)"] <- coord_value
          } else if (var_name == "Middle income ($40,000-$79,999)") {
            boot_weights[b, "Income_Middle income ($40,000-$79,999)"] <- coord_value
          } else if (var_name == "Stay-at-home caregiver") {
            boot_weights[b, "Employment_Stay-at-home caregiver"] <- coord_value
          } else if (var_name == "Student") {
            boot_weights[b, "Employment_Student"] <- coord_value
          } else if (var_name == "Unemployed") {
            boot_weights[b, "Employment_Unemployed"] <- coord_value
          } else if (var_name == "Cohabiting") {
            boot_weights[b, "Family_Cohabiting"] <- coord_value
          } else if (var_name == "Single") {
            boot_weights[b, "Family_Single"] <- coord_value
          }
        }
      }, error = function(e) {
        # If bootstrap fails, use original weights
        boot_weights[b, ] <- full_weights
      })
      
    } else if (model_name == "MCA_Dim1") {
      # Prepare data for MCA
      mca_boot_data <- data.frame(
        mat_age_cat = factor(ifelse(boot_data$mat_age_young == 1, "Young (≤25)", "Older (>25)"),
                             levels = c("Young (≤25)", "Older (>25)")),
        Education = apply(boot_data[, education_predictors], 1, function(x) {
          if(sum(x) == 0) return(reference_groups$education)
          education_predictors[which(x == 1)]
        }),
        Income = apply(boot_data[, income_predictors], 1, function(x) {
          if(sum(x) == 0) return(reference_groups$income)
          income_predictors[which(x == 1)]
        }),
        Employment = apply(boot_data[, employment_predictors], 1, function(x) {
          if(sum(x) == 0) return(reference_groups$employment)
          employment_predictors[which(x == 1)]
        }),
        Family = apply(boot_data[, family_predictors], 1, function(x) {
          if(sum(x) == 0) return(reference_groups$family)
          family_predictors[which(x == 1)]
        })
      )
      
      # Clean factor names
      for(col in c("Education", "Income", "Employment", "Family")) {
        mca_boot_data[[col]] <- gsub(".*_", "", mca_boot_data[[col]])
        mca_boot_data[[col]] <- factor(mca_boot_data[[col]])
      }
      
      # Run MCA on bootstrap sample
      tryCatch({
        boot_mca <- MCA(mca_boot_data, graph = FALSE, ncp = 1)
        boot_coords <- boot_mca$var$coord[, 1]
        
        # Get correlation for sign
        boot_cor <- cor(boot_mca$ind$coord[, 1], boot_data[[outcome]], use = "complete.obs")
        
        # Extract weights for this bootstrap sample
        for (var_name in names(boot_coords)) {
          coord_value <- boot_coords[var_name] * sign(boot_cor)
          
          if (grepl("\\.", var_name)) {
            parts <- strsplit(var_name, "\\.")[[1]]
            var_type <- parts[1]
            level <- paste(parts[-1], collapse = ".")
            
            if (var_type == "mat_age_cat" && level == "Young (≤25)") {
              boot_weights[b, "mat_age_young"] <- coord_value
            } else if (var_type == "Education" && level == "College degree") {
              boot_weights[b, "Education_College degree"] <- coord_value
            } else if (var_type == "Education" && level == "High school or less") {
              boot_weights[b, "Education_High school or less"] <- coord_value
            } else if (var_type == "Income" && grepl("Low income", level)) {
              boot_weights[b, "Income_Low income (<$40,000)"] <- coord_value
            } else if (var_type == "Income" && grepl("Middle income", level)) {
              boot_weights[b, "Income_Middle income ($40,000-$79,999)"] <- coord_value
            } else if (var_type == "Employment" && grepl("Stay.*home", level)) {
              boot_weights[b, "Employment_Stay-at-home caregiver"] <- coord_value
            } else if (var_type == "Employment" && level == "Student") {
              boot_weights[b, "Employment_Student"] <- coord_value
            } else if (var_type == "Employment" && level == "Unemployed") {
              boot_weights[b, "Employment_Unemployed"] <- coord_value
            } else if (var_type == "Family" && level == "Cohabiting") {
              boot_weights[b, "Family_Cohabiting"] <- coord_value
            } else if (var_type == "Family" && level == "Single") {
              boot_weights[b, "Family_Single"] <- coord_value
            }
          }
        }
      }, error = function(e) {
        # If bootstrap fails, use original weights
        boot_weights[b, ] <- full_weights
      })
    }
  }
  
  # Calculate statistics from bootstrap samples
  weight_stats <- data.frame(
    Variable = names(full_weights),
    Mean = full_weights,
    SD = apply(boot_weights, 2, sd, na.rm = TRUE),
    CI_Lower = apply(boot_weights, 2, quantile, probs = 0.025, na.rm = TRUE),
    CI_Upper = apply(boot_weights, 2, quantile, probs = 0.975, na.rm = TRUE),
    stringsAsFactors = FALSE
  )
  
  # Calculate p-values based on bootstrap distribution
  weight_stats$p_value <- sapply(1:nrow(weight_stats), function(i) {
    if (abs(weight_stats$Mean[i]) < 1e-10) {
      return(1)
    }
    # Calculate proportion of bootstrap samples with opposite sign
    boot_vals <- boot_weights[, i]
    if (weight_stats$Mean[i] > 0) {
      p_val <- mean(boot_vals <= 0)
    } else {
      p_val <- mean(boot_vals >= 0)
    }
    return(2 * min(p_val, 1 - p_val))  # Two-tailed test
  })
  
  # Add additional statistics
  weight_stats$SE <- weight_stats$SD
  weight_stats$t_stat <- ifelse(weight_stats$SE > 0 & !is.na(weight_stats$SE), 
                                weight_stats$Mean / weight_stats$SE, 0)
  
  # Diagnostic output
  cat("    Bootstrap CI range check:\n")
  non_zero_idx <- which(abs(weight_stats$Mean) > 0.001)
  if (length(non_zero_idx) > 0) {
    for (i in non_zero_idx[1:min(3, length(non_zero_idx))]) {
      cat("      ", weight_stats$Variable[i], ": [", 
          round(weight_stats$CI_Lower[i], 3), ", ", 
          round(weight_stats$CI_Upper[i], 3), "]\n", sep = "")
    }
  }
  
  cat("    Non-zero weights:", sum(abs(weight_stats$Mean) > 0.001), 
      "out of", length(full_weights), "\n")
  
  return(weight_stats)
}

# Function to get all pairwise comparisons for categorical variables
get_all_pairwise_comparisons <- function(data, model_name, outcome, n_boot = 1000) {
  
  cat("    Calculating pairwise comparisons for", model_name, "model...\n")
  
  # Define the categorical variable groups and their levels
  category_groups <- list(
    Education = list(
      levels = c("Education_College degree", "Education_High school or less"),
      reference = "Education_Graduate degree and higher"
    ),
    Income = list(
      levels = c("Income_Low income (<$40,000)", "Income_Middle income ($40,000-$79,999)"),
      reference = "Income_High income (>=$80,000)"
    ),
    Employment = list(
      levels = c("Employment_Stay-at-home caregiver", "Employment_Student", "Employment_Unemployed"),
      reference = "Employment_Employed"
    ),
    Family = list(
      levels = c("Family_Cohabiting", "Family_Single"),
      reference = "Family_Married"
    )
  )
  
  # Storage for all comparisons
  all_comparisons <- list()
  
  # First fit the model on full data to get baseline estimates
  if (model_name == "ridge") {
    X_full <- model.matrix(~ . - 1, data[predictors])
    y_full <- data[[outcome]]
    ridge_fit <- glmnet(X_full, y_full, alpha = 0, lambda = 0.001, standardize = TRUE)
    full_coefs <- as.numeric(coef(ridge_fit))[-1]
    names(full_coefs) <- predictors
    
  } else if (model_name == "lasso") {
    X_full <- model.matrix(~ . - 1, data[predictors])
    y_full <- data[[outcome]]
    cv_lasso <- cv.glmnet(X_full, y_full, alpha = 1, standardize = TRUE, nfolds = 10)
    full_coefs <- as.numeric(coef(cv_lasso, s = "lambda.min"))[-1]
    names(full_coefs) <- predictors
    
  } else if (model_name == "elastic") {
    X_full <- model.matrix(~ . - 1, data[predictors])
    y_full <- data[[outcome]]
    cv_elastic <- cv.glmnet(X_full, y_full, alpha = 0.5, standardize = TRUE, nfolds = 10)
    full_coefs <- as.numeric(coef(cv_elastic, s = "lambda.min"))[-1]
    names(full_coefs) <- predictors
    
  } else if (model_name == "ols") {
    ols_fit <- lm(as.formula(paste(outcome, "~ .")), data = data[c(predictors, outcome)])
    full_coefs <- coef(ols_fit)[-1]
    full_coefs[is.na(full_coefs)] <- 0
    
  } else if (model_name == "rf") {
    # RF doesn't provide linear coefficients for comparisons
    cat("      Note: Random Forest doesn't provide linear coefficients for pairwise comparisons\n")
    return(data.frame(
      Comparison = character(),
      Estimate = numeric(),
      SE = numeric(),
      CI_Lower = numeric(),
      CI_Upper = numeric(),
      p_value = numeric(),
      Significance = character(),
      stringsAsFactors = FALSE
    ))
  }
  
  # Add reference level coefficient as 0
  full_coefs_with_ref <- c(full_coefs, 
                           "Education_Graduate degree and higher" = 0,
                           "Income_High income (>=$80,000)" = 0,
                           "Employment_Employed" = 0,
                           "Family_Married" = 0)
  
  # Calculate pairwise differences for each category
  for (category_name in names(category_groups)) {
    cat_info <- category_groups[[category_name]]
    all_levels <- c(cat_info$levels, cat_info$reference)
    
    # Get all pairwise combinations
    for (i in 1:(length(all_levels) - 1)) {
      for (j in (i + 1):length(all_levels)) {
        level1 <- all_levels[i]
        level2 <- all_levels[j]
        
        # Calculate difference
        coef1 <- ifelse(level1 %in% names(full_coefs_with_ref), full_coefs_with_ref[level1], 0)
        coef2 <- ifelse(level2 %in% names(full_coefs_with_ref), full_coefs_with_ref[level2], 0)
        diff_estimate <- coef1 - coef2
        
        # Create comparison name
        level1_clean <- gsub(".*_", "", level1)
        level2_clean <- gsub(".*_", "", level2)
        comparison_name <- paste0(category_name, ": ", level1_clean, " vs ", level2_clean)
        
        all_comparisons[[comparison_name]] <- list(
          estimate = diff_estimate,
          level1 = level1,
          level2 = level2,
          category = category_name
        )
      }
    }
  }
  
  # Bootstrap for confidence intervals
  boot_diffs <- matrix(NA, nrow = n_boot, ncol = length(all_comparisons))
  colnames(boot_diffs) <- names(all_comparisons)
  
  for (b in 1:n_boot) {
    boot_idx <- sample(nrow(data), replace = TRUE)
    boot_data <- data[boot_idx, ]
    
    tryCatch({
      if (model_name == "ridge") {
        X_boot <- model.matrix(~ . - 1, boot_data[predictors])
        y_boot <- boot_data[[outcome]]
        ridge_boot <- glmnet(X_boot, y_boot, alpha = 0, lambda = 0.001, standardize = TRUE)
        boot_coefs <- as.numeric(coef(ridge_boot))[-1]
        names(boot_coefs) <- predictors
        
      } else if (model_name == "lasso") {
        X_boot <- model.matrix(~ . - 1, boot_data[predictors])
        y_boot <- boot_data[[outcome]]
        cv_boot <- cv.glmnet(X_boot, y_boot, alpha = 1, standardize = TRUE, nfolds = 5)
        boot_coefs <- as.numeric(coef(cv_boot, s = "lambda.min"))[-1]
        names(boot_coefs) <- predictors
        
      } else if (model_name == "elastic") {
        X_boot <- model.matrix(~ . - 1, boot_data[predictors])
        y_boot <- boot_data[[outcome]]
        cv_boot <- cv.glmnet(X_boot, y_boot, alpha = 0.5, standardize = TRUE, nfolds = 5)
        boot_coefs <- as.numeric(coef(cv_boot, s = "lambda.min"))[-1]
        names(boot_coefs) <- predictors
        
      } else if (model_name == "ols") {
        lm_boot <- lm(as.formula(paste(outcome, "~ .")), data = boot_data[c(predictors, outcome)])
        boot_coefs <- coef(lm_boot)[-1]
        boot_coefs[is.na(boot_coefs)] <- 0
      }
      
      # Add reference levels
      boot_coefs_with_ref <- c(boot_coefs,
                               "Education_Graduate degree and higher" = 0,
                               "Income_High income (>=$80,000)" = 0,
                               "Employment_Employed" = 0,
                               "Family_Married" = 0)
      
      # Calculate bootstrap differences
      for (comp_idx in 1:length(all_comparisons)) {
        comp_info <- all_comparisons[[comp_idx]]
        coef1 <- ifelse(comp_info$level1 %in% names(boot_coefs_with_ref), 
                        boot_coefs_with_ref[comp_info$level1], 0)
        coef2 <- ifelse(comp_info$level2 %in% names(boot_coefs_with_ref), 
                        boot_coefs_with_ref[comp_info$level2], 0)
        boot_diffs[b, comp_idx] <- coef1 - coef2
      }
      
    }, error = function(e) {
      # Skip this bootstrap sample if it fails
      boot_diffs[b, ] <- NA
    })
  }
  
  # Remove failed bootstrap samples
  boot_diffs <- boot_diffs[complete.cases(boot_diffs), , drop = FALSE]
  
  # Create results data frame
  results <- data.frame(
    Comparison = names(all_comparisons),
    Estimate = sapply(all_comparisons, function(x) x$estimate),
    SE = apply(boot_diffs, 2, sd, na.rm = TRUE),
    CI_Lower = apply(boot_diffs, 2, quantile, probs = 0.025, na.rm = TRUE),
    CI_Upper = apply(boot_diffs, 2, quantile, probs = 0.975, na.rm = TRUE),
    stringsAsFactors = FALSE
  )
  
  # Calculate p-values based on bootstrap distribution
  results$p_value <- sapply(1:ncol(boot_diffs), function(i) {
    if (abs(results$Estimate[i]) < 1e-10) {
      return(1)
    }
    # Proportion of bootstrap samples with opposite sign
    if (results$Estimate[i] > 0) {
      p_neg <- mean(boot_diffs[, i] <= 0)
    } else {
      p_neg <- mean(boot_diffs[, i] >= 0)
    }
    # Two-tailed p-value
    return(2 * min(p_neg, 1 - p_neg))
  })
  
  # Add significance stars
  results$Significance <- case_when(
    results$p_value < 0.001 ~ "***",
    results$p_value < 0.01 ~ "**",
    results$p_value < 0.05 ~ "*",
    TRUE ~ ""
  )
  
  # Sort by category and comparison
  results <- results %>%
    arrange(Comparison)
  
  cat("      Completed", nrow(results), "pairwise comparisons\n")
  cat("      Significant comparisons (p < 0.05):", sum(results$p_value < 0.05), "\n")
  
  return(results)
}
# 6. Perform nested CV for all outcomes ----
perform_nested_cv_all_outcomes <- function(data,
                                           supervised_wf,
                                           unsupervised_wf,
                                           n_outer = 10,
                                           n_inner = 5) {
  
  all_results <- list()
  
  for (outcome_name in outcomes) {
    cat("\n========================================\n")
    cat("Evaluating outcome:", outcome_name, "\n")
    cat("========================================\n")
    
    # Prepare outcome-specific data
    outcome_data <- data %>%
      dplyr::select(-dplyr::all_of(setdiff(outcomes, outcome_name))) %>%
      dplyr::rename(outcome = dplyr::all_of(outcome_name))
    
    # Recipe for supervised models - NO NEED TO REMOVE REFERENCE CATEGORIES
    # They're already excluded from predictors
    supervised_recipe <- recipe(outcome ~ ., data = outcome_data) %>%
      update_role(study_id, mat_age, new_role = "ID") %>%
      update_role(FAMD_Dim1, MCA_Dim1, PAM_Cluster, new_role = "unsupervised") %>%
      step_rm(FAMD_Dim1, MCA_Dim1, PAM_Cluster) %>%
      step_nzv(all_predictors())
    
    # Outer CV folds
    outer_folds <- vfold_cv(outcome_data, v = n_outer, strata = outcome)
    
    outcome_results <- list()
    
    # Process supervised models
    cat("\nEvaluating supervised models...\n")
    for (model_name in names(supervised_wf)) {
      
      cat("  Training", model_name, "...\n")
      current_wf <- supervised_wf[[model_name]] %>% add_recipe(supervised_recipe)
      
      # Handle tuning
      if (model_name %in% c("ridge", "lasso", "elastic", "rf")) {
        
        # Tuning grids
        tune_grid <- switch(
          model_name,
          ridge   = grid_regular(penalty(range = c(-5, 0), trans = log10_trans()),
                                 levels = 20),
          lasso   = grid_regular(penalty(range = c(-5, -1), trans = log10_trans()),
                                 levels = 20),
          elastic = grid_regular(
            penalty(range = c(-5, -1), trans = log10_trans()),
            mixture(range = c(0.1, 0.9)),
            levels = c(15, 5)),
          rf      = grid_regular(
            mtry(range  = c(2, min(length(predictors) - 1, 8))),
            min_n(range = c(2, 20)),
            levels = c(5, 3))
        )
        
        nested_results <- outer_folds %>%
          mutate(
            inner     = map(splits, ~ vfold_cv(analysis(.x), v = n_inner, strata = outcome)),
            tuned     = map(inner,  ~ tune_grid(
              current_wf, resamples = .x, grid = tune_grid,
              metrics = metric_set(rmse, rsq, mae),
              control = control_grid(save_pred = TRUE, verbose = FALSE, parallel_over = "everything"))),
            best      = map(tuned,  ~ select_best(.x, metric = "rsq")),
            final_wf  = map(best,   ~ finalize_workflow(current_wf, .x)),
            fit       = map2(final_wf, splits, ~ fit(.x, analysis(.y))),
            pred      = map2(fit, splits,      ~ augment(.x, new_data = assessment(.y)))
          )
        
      } else {  # OLS
        nested_results <- outer_folds %>%
          mutate(
            fit  = map(splits, ~ suppressWarnings(fit(current_wf, analysis(.x)))),
            pred = map2(fit, splits, ~ augment(.x, assessment(.y)))
          )
      }
      
      # Calculate metrics
      test_metrics <- nested_results %>%
        dplyr::select(id, pred) %>%
        unnest(pred) %>%
        group_by(id) %>%
        metrics(truth = outcome, estimate = .pred) %>%
        group_by(.metric) %>%
        summarise(
          mean   = mean(.estimate, na.rm = TRUE),
          sd     = sd(.estimate,   na.rm = TRUE),
          median = median(.estimate, na.rm = TRUE),
          q25    = quantile(.estimate, 0.25, na.rm = TRUE),
          q75    = quantile(.estimate, 0.75, na.rm = TRUE),
          .groups = "drop"
        ) %>%
        mutate(model   = model_name,
               outcome = outcome_name,
               type    = "supervised")
      
      outcome_results[[model_name]] <- list(
        metrics = test_metrics,
        nested  = nested_results,
        type    = "supervised"
      )
    }
    
    # Process unsupervised models
    cat("\nEvaluating unsupervised models...\n")
    for (model_name in names(unsupervised_wf)) {
      
      cat("  Training", model_name, "...\n")
      
      # Create recipe using only the specific unsupervised feature
      unsup_recipe <- recipe(outcome ~ ., data = outcome_data) %>%
        update_role(study_id, mat_age, new_role = "ID") %>%
        step_rm(all_of(predictors)) %>%
        step_rm(all_of(setdiff(c("FAMD_Dim1", "MCA_Dim1", "PAM_Cluster"), model_name)))
      
      current_wf <- unsupervised_wf[[model_name]] %>% add_recipe(unsup_recipe)
      
      nested_results <- outer_folds %>%
        mutate(
          fit  = map(splits, ~ fit(current_wf, analysis(.x))),
          pred = map2(fit, splits, ~ augment(.x, assessment(.y)))
        )
      
      # Calculate metrics
      test_metrics <- nested_results %>%
        dplyr::select(id, pred) %>%
        unnest(pred) %>%
        group_by(id) %>%
        metrics(truth = outcome, estimate = .pred) %>%
        group_by(.metric) %>%
        summarise(
          mean   = mean(.estimate, na.rm = TRUE),
          sd     = sd(.estimate,   na.rm = TRUE),
          median = median(.estimate, na.rm = TRUE),
          q25    = quantile(.estimate, 0.25, na.rm = TRUE),
          q75    = quantile(.estimate, 0.75, na.rm = TRUE),
          .groups = "drop"
        ) %>%
        mutate(model   = model_name,
               outcome = outcome_name,
               type    = "unsupervised")
      
      outcome_results[[model_name]] <- list(
        metrics = test_metrics,
        nested  = nested_results,
        type    = "unsupervised"
      )
    }
    
    all_results[[outcome_name]] <- outcome_results
  }
  
  all_results
}

# 7. Create workflows ----
cat("\nCreating workflows...\n")

supervised_workflows <- list(
  ols     = workflow() %>% add_model(ols_spec),
  ridge   = workflow() %>% add_model(ridge_spec),
  lasso   = workflow() %>% add_model(lasso_spec),
  elastic = workflow() %>% add_model(elastic_spec),
  rf      = workflow() %>% add_model(rf_spec)
)

unsupervised_workflows <- list(
  FAMD_Dim1   = workflow() %>% add_model(unsup_spec),
  MCA_Dim1    = workflow() %>% add_model(unsup_spec),
  PAM_Cluster = workflow() %>% add_model(unsup_spec)
)

# 8. Run nested CV ----
cat("\nRunning nested cross-validation for all outcomes...\n")

all_cv_results <- perform_nested_cv_all_outcomes(df_imputed,
                                                 supervised_workflows,
                                                 unsupervised_workflows)

# 9. Extract results and identify best models ----
cat("\nExtracting results and identifying best models...\n")

# Combine all metrics
all_metrics <- map_df(outcomes, function(outcome) {
  map_df(all_cv_results[[outcome]], function(model_result) {
    model_result$metrics
  })
})

# Identify best models based on cognitive outcome
cognitive_rsq <- all_metrics %>%
  filter(.metric == "rsq", outcome == primary_outcome) %>%
  select(outcome, model, mean, sd, type) %>%
  arrange(type, desc(mean))

best_supervised <- cognitive_rsq %>%
  filter(type == "supervised") %>%
  arrange(desc(mean)) %>%
  slice(1) %>%
  pull(model)

best_unsupervised <- cognitive_rsq %>%
  filter(type == "unsupervised") %>%
  arrange(desc(mean)) %>%
  slice(1) %>%
  pull(model)

cat("\n=== BEST MODELS (based on cognitive outcome) ===\n")
cat("Best supervised model:", best_supervised, "\n")
cat("Best unsupervised model:", best_unsupervised, "\n\n")

# 10. Extract weights for all outcomes using best models ----
cat("\nExtracting weights with confidence intervals for all outcomes...\n")

all_weights <- list()
all_pairwise_comparisons <- list()

for (outcome in outcomes) {
  cat("\nProcessing weights for", outcome, "...\n")
  
  # Extract supervised weights
  supervised_weights <- extract_model_weights(df_imputed, best_supervised, outcome, n_boot = 1000)
  supervised_weights$Model <- best_supervised
  supervised_weights$Type <- "Supervised"
  supervised_weights$Outcome <- outcome
  supervised_weights$Direction <- ifelse(supervised_weights$Mean > 0, "Protective", "Risk")
  
  # Extract unsupervised weights (with reduced bootstrap for computational efficiency)
  unsupervised_weights <- extract_unsupervised_weights(df_imputed, best_unsupervised, outcome, n_boot = 200)
  unsupervised_weights$Model <- best_unsupervised
  unsupervised_weights$Type <- "Unsupervised"
  unsupervised_weights$Outcome <- outcome
  unsupervised_weights$Direction <- ifelse(unsupervised_weights$Mean > 0, "Protective", "Risk")
  
  # Get all pairwise comparisons for supervised model
  pairwise_comparisons <- get_all_pairwise_comparisons(df_imputed, best_supervised, outcome, n_boot = 1000)
  pairwise_comparisons$Outcome <- outcome
  pairwise_comparisons$Model <- best_supervised
  
  all_weights[[outcome]] <- list(
    supervised = supervised_weights,
    unsupervised = unsupervised_weights
  )
  
  all_pairwise_comparisons[[outcome]] <- pairwise_comparisons
}

# 10.1 Quality check for confidence intervals ----
cat("\n\nCONFIDENCE INTERVAL QUALITY CHECK:\n")
cat("===================================\n")

for (outcome in outcomes) {
  cat("\n", outcome, ":\n", sep = "")
  
  # Check supervised model CI quality
  sup_weights <- all_weights[[outcome]]$supervised
  ci_cross <- sum((sup_weights$CI_Lower < 0) & (sup_weights$CI_Upper > 0))
  sig_despite_cross <- sum((sup_weights$CI_Lower < 0) & (sup_weights$CI_Upper > 0) & 
                             (sup_weights$p_value < 0.05))
  
  cat("  Supervised Model (", best_supervised, "):\n", sep = "")
  cat("    Total predictors: ", nrow(sup_weights), "\n", sep = "")
  cat("    CIs crossing zero: ", ci_cross, " (", 
      round(100 * ci_cross / nrow(sup_weights), 1), "%)\n", sep = "")
  cat("    Significant predictors: ", sum(sup_weights$p_value < 0.05), "\n", sep = "")
  if (sig_despite_cross > 0) {
    cat("    WARNING: ", sig_despite_cross, " significant despite CI crossing zero\n", sep = "")
  }
  
  # Check CI width variation
  ci_widths <- sup_weights$CI_Upper - sup_weights$CI_Lower
  cat("    CI width range: [", round(min(ci_widths, na.rm = TRUE), 3), ", ", 
      round(max(ci_widths, na.rm = TRUE), 3), "]\n", sep = "")
  
  # Report any issues with extraction
  if (any(is.na(sup_weights$CI_Lower)) || any(is.na(sup_weights$CI_Upper))) {
    cat("    WARNING: Some CIs could not be calculated\n")
  }
  
  # Check unsupervised model CI quality
  unsup_weights <- all_weights[[outcome]]$unsupervised
  unsup_nonzero <- unsup_weights[abs(unsup_weights$Mean) > 0.001, ]
  
  if (nrow(unsup_nonzero) > 0) {
    ci_cross_unsup <- sum((unsup_nonzero$CI_Lower < 0) & (unsup_nonzero$CI_Upper > 0))
    
    cat("  Unsupervised Model (", best_unsupervised, "):\n", sep = "")
    cat("    Non-zero weights: ", nrow(unsup_nonzero), "/", nrow(unsup_weights), "\n", sep = "")
    cat("    CIs crossing zero: ", ci_cross_unsup, " (", 
        round(100 * ci_cross_unsup / nrow(unsup_nonzero), 1), "%)\n", sep = "")
    cat("    Significant non-zero weights: ", 
        sum(unsup_nonzero$p_value < 0.05), "\n", sep = "")
    
    # Check CI width variation for unsupervised
    unsup_ci_widths <- unsup_nonzero$CI_Upper - unsup_nonzero$CI_Lower
    if (length(unsup_ci_widths) > 0 && all(!is.na(unsup_ci_widths))) {
      cat("    CI width range: [", round(min(unsup_ci_widths), 3), ", ", 
          round(max(unsup_ci_widths), 3), "]\n", sep = "")
      
      # Check if all CIs are identical (would indicate no variation in bootstrap)
      if (sd(unsup_ci_widths) < 0.001) {
        cat("    WARNING: All CI widths are nearly identical - check bootstrap procedure\n")
      }
    }
  } else {
    cat("  Unsupervised Model (", best_unsupervised, "):\n", sep = "")
    cat("    WARNING: No non-zero weights extracted!\n")
    cat("    This may indicate an issue with variable name mapping.\n")
  }
  
  # Check pairwise comparisons
  if (outcome %in% names(all_pairwise_comparisons)) {
    pairwise_data <- all_pairwise_comparisons[[outcome]]
    cat("  Pairwise Comparisons:\n")
    cat("    Total comparisons: ", nrow(pairwise_data), "\n", sep = "")
    cat("    Significant comparisons: ", sum(pairwise_data$p_value < 0.05), "\n", sep = "")
  }
}

# 11. Create visualizations ----
cat("\nCreating visualizations...\n")

# Prepare data for plotting with better model names
plot_metrics <- all_metrics %>%
  filter(.metric == "rsq") %>%
  mutate(
    se = sd / sqrt(10),
    outcome = factor(outcome, levels = outcomes),
    outcome_label = case_when(
      outcome == "cog_comp_score" ~ "Cognitive",
      outcome == "lang_comp_score" ~ "Language",
      outcome == "motor_comp_score" ~ "Motor"
    ),
    model_label = case_when(
      model == "ols" ~ "OLS",
      model == "ridge" ~ "Ridge",
      model == "lasso" ~ "Lasso",
      model == "elastic" ~ "Elastic Net",
      model == "rf" ~ "Random Forest",
      model == "FAMD_Dim1" ~ "FAMD",
      model == "MCA_Dim1" ~ "MCA",
      model == "PAM_Cluster" ~ "PAM Clustering"
    ),
    model_type_label = case_when(
      type == "supervised" ~ "Supervised",
      type == "unsupervised" ~ "Unsupervised"
    ),
    is_best = (type == "supervised" & model == best_supervised) |
      (type == "unsupervised" & model == best_unsupervised)
  )

# Create integrated model comparison plot with border highlighting for best models
p_integrated <- ggplot(plot_metrics, aes(x = model_label, y = mean, fill = model_type_label)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  # Add thick border for best models
  geom_bar(data = filter(plot_metrics, is_best),
           aes(x = model_label, y = mean, fill = model_type_label),
           stat = "identity", position = "dodge", alpha = 0,
           color = "orange", linewidth = 1.2) +
  geom_errorbar(aes(ymin = pmax(0, mean - se), ymax = mean + se),
                position = position_dodge(0.9), width = 0.25, linewidth = 0.8) +
  facet_wrap(~ outcome_label, scales = "free_x", ncol = 3) +
  scale_fill_manual(values = c("Supervised" = "#2E86AB", "Unsupervised" = "#A23B72"),
                    name = "Model Type") +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.1))) +
  labs(x = "Model", y = expression(R^2)) +
  theme_classic(base_size = 16) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
    axis.text.y = element_text(size = 14),
    axis.title = element_text(size = 16, face = "bold"),
    legend.position = "top",
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14, face = "bold"),
    strip.text = element_text(size = 16, face = "bold"),
    strip.background = element_blank(),
    panel.grid.major.y = element_line(color = "grey90", linewidth = 0.5),
    panel.spacing = unit(2, "lines")
  )  
ggsave("all_outcomes_model_comparison.png", p_integrated, width = 14, height = 8, dpi = 300)

# Create weight plots for each outcome
for (outcome in outcomes) {
  
  # Supervised weights plot
  sup_weights <- all_weights[[outcome]]$supervised %>%
    mutate(
      Variable_clean = case_when(
        Variable == "mat_age_young" ~ "Maternal age ≤25",
        TRUE ~ gsub("_", " ", Variable)
      )
    ) %>%
    arrange(desc(abs(Mean))) %>%
    mutate(Variable_clean = factor(Variable_clean, levels = Variable_clean))
  
  p_weights_sup <- ggplot(sup_weights, aes(x = Variable_clean, y = Mean)) +
    geom_bar(stat = "identity", aes(fill = Direction), alpha = 0.8) +
    geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.3) +
    geom_text(aes(label = ifelse(p_value < 0.001, "***",
                                 ifelse(p_value < 0.01, "**",
                                        ifelse(p_value < 0.05, "*", "")))),
              vjust = -0.5, size = 4) +
    coord_flip() +
    scale_fill_manual(values = c("Protective" = "#2E7D32", "Risk" = "#D32F2F")) +
    labs(x = NULL, y = "Weight (with 95% CI)") +
    theme_minimal(base_size = 11) +
    theme(legend.position = "top")
  
  ggsave(paste0(outcome, "_weights_with_ci.png"), p_weights_sup, width = 10, height = 8, dpi = 300)
  
  # Unsupervised weights plot
  unsup_weights <- all_weights[[outcome]]$unsupervised %>%
    filter(abs(Mean) > 0.001) %>%
    mutate(
      Variable_clean = case_when(
        Variable == "mat_age_young" ~ "Maternal age ≤25",
        TRUE ~ gsub("_", " ", Variable)
      )
    ) %>%
    arrange(desc(abs(Mean))) %>%
    mutate(Variable_clean = factor(Variable_clean, levels = Variable_clean))
  
  if (nrow(unsup_weights) > 0) {
    p_weights_unsup <- ggplot(unsup_weights, aes(x = Variable_clean, y = Mean)) +
      geom_bar(stat = "identity", aes(fill = Direction), alpha = 0.8) +
      geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.3) +
      geom_text(aes(label = ifelse(p_value < 0.001, "***",
                                   ifelse(p_value < 0.01, "**",
                                          ifelse(p_value < 0.05, "*", "")))),
                vjust = -0.5, size = 4) +
      coord_flip() +
      scale_fill_manual(values = c("Protective" = "#2E7D32", "Risk" = "#D32F2F")) +
      labs(x = NULL, y = "Weight (with 95% CI)",
           subtitle = paste("Unsupervised model:", best_unsupervised)) +
      theme_minimal(base_size = 11) +
      theme(legend.position = "top")
    
    ggsave(paste0(outcome, "_unsupervised_weights_with_ci.png"), p_weights_unsup,
           width = 10, height = 8, dpi = 300)
  }
}

# 12. Calculate SRC scores ----
cat("\nCalculating SRC scores for all outcomes...\n")

src_scores <- data.frame(study_id = df_imputed$study_id)

for (outcome in outcomes) {
  cat("\nCalculating scores for", outcome, "...\n")
  
  # Get weights for this outcome
  sup_weights <- all_weights[[outcome]]$supervised
  weight_vector <- setNames(sup_weights$Mean, sup_weights$Variable)
  
  # Calculate supervised score
  X_matrix <- as.matrix(df_imputed[names(weight_vector)])
  supervised_score <- as.numeric(X_matrix %*% weight_vector)
  
  # Calculate unsupervised score
  if (best_unsupervised == "FAMD_Dim1") {
    unsupervised_score <- df_imputed$FAMD_Dim1 * 
      cor(df_imputed$FAMD_Dim1, df_imputed[[outcome]], use = "complete.obs")
  } else if (best_unsupervised == "MCA_Dim1") {
    unsupervised_score <- df_imputed$MCA_Dim1 * 
      cor(df_imputed$MCA_Dim1, df_imputed[[outcome]], use = "complete.obs")
  } else if (best_unsupervised == "PAM_Cluster") {
    unsupervised_score <- numeric(nrow(df_imputed))
    for (i in 1:optimal_k) {
      cluster_mask <- df_imputed$PAM_Cluster == i
      cluster_cor <- cor(as.numeric(cluster_mask), df_imputed[[outcome]], use = "complete.obs")
      unsupervised_score[cluster_mask] <- cluster_cor
    }
  }
  
  # Add to dataframe
  src_scores[[paste0(outcome, "_supervised")]] <- supervised_score
  src_scores[[paste0(outcome, "_unsupervised")]] <- unsupervised_score
  
  # Calculate risk tertiles
  src_scores[[paste0(outcome, "_sup_tertile")]] <- cut(
    supervised_score,
    breaks = quantile(supervised_score, c(0, 1/3, 2/3, 1)),
    include.lowest = TRUE,
    labels = c("High Risk", "Moderate Risk", "Low Risk")
  )
  
  src_scores[[paste0(outcome, "_unsup_tertile")]] <- cut(
    unsupervised_score,
    breaks = quantile(unsupervised_score, c(0, 1/3, 2/3, 1)),
    include.lowest = TRUE,
    labels = c("High Risk", "Moderate Risk", "Low Risk")
  )
}

# Add original outcomes
src_scores <- cbind(src_scores, df_imputed[outcomes])

# 13. Export comprehensive results ----
cat("\nExporting comprehensive results...\n")

# Function to format numbers
format_number <- function(x, digits = 3) {
  as.character(sapply(x, function(val) {
    if (is.na(val)) return(NA_character_)
    if (abs(val) < 0.001 && val != 0) {
      return(sprintf("%.6f", val))
    } else {
      return(sprintf(paste0("%.", digits, "f"), val))
    }
  }))
}

# Export model performance
model_perf_export <- all_metrics %>% 
  filter(.metric == "rsq") %>%
  mutate(across(where(is.numeric), ~format_number(.x, 3)))

write_csv(model_perf_export, paste0("SRC_Model_Performance_", best_supervised, "_", best_unsupervised, ".csv"))

# Export weights with CIs for all outcomes
all_weights_export <- bind_rows(
  lapply(outcomes, function(outcome) {
    # Process supervised weights
    sup_weights <- all_weights[[outcome]]$supervised %>%
      mutate(
        Variable_clean = case_when(
          Variable == "mat_age_young" ~ "Maternal age ≤25",
          TRUE ~ Variable
        ),
        Mean_formatted = format_number(Mean, 3),
        SD_formatted = format_number(SD, 3),
        CI_Lower_formatted = format_number(CI_Lower, 3),
        CI_Upper_formatted = format_number(CI_Upper, 3),
        p_value_formatted = format_number(p_value, 4),
        Weight_CI = paste0(
          format_number(Mean, 3), " [",
          format_number(CI_Lower, 3), ", ",
          format_number(CI_Upper, 3), "]"
        ),
        Significance = case_when(
          p_value < 0.001 ~ "***",
          p_value < 0.01 ~ "**",
          p_value < 0.05 ~ "*",
          TRUE ~ ""
        )
      ) %>%
      select(Outcome, Variable = Variable_clean,
             Mean = Mean_formatted,
             SD = SD_formatted,
             CI_Lower = CI_Lower_formatted,
             CI_Upper = CI_Upper_formatted,
             p_value = p_value_formatted,
             Significance, Direction, Model, Type, Weight_CI)
    
    # Process unsupervised weights
    unsup_weights <- all_weights[[outcome]]$unsupervised %>%
      mutate(
        Variable_clean = case_when(
          Variable == "mat_age_young" ~ "Maternal age ≤25",
          TRUE ~ Variable
        ),
        Mean_formatted = format_number(Mean, 3),
        SD_formatted = format_number(SD, 3),
        CI_Lower_formatted = format_number(CI_Lower, 3),
        CI_Upper_formatted = format_number(CI_Upper, 3),
        p_value_formatted = format_number(p_value, 4),
        Weight_CI = paste0(
          format_number(Mean, 3), " [",
          format_number(CI_Lower, 3), ", ",
          format_number(CI_Upper, 3), "]"
        ),
        Significance = case_when(
          p_value < 0.001 ~ "***",
          p_value < 0.01 ~ "**",
          p_value < 0.05 ~ "*",
          TRUE ~ ""
        )
      ) %>%
      select(Outcome, Variable = Variable_clean,
             Mean = Mean_formatted,
             SD = SD_formatted,
             CI_Lower = CI_Lower_formatted,
             CI_Upper = CI_Upper_formatted,
             p_value = p_value_formatted,
             Significance, Direction, Model, Type, Weight_CI)
    
    bind_rows(sup_weights, unsup_weights)
  })
)

# Export all weights to single comprehensive file
write_csv(all_weights_export, "SRC_All_Weights_With_CI.csv")

# Export supervised weights separately
supervised_weights_export <- all_weights_export %>%
  filter(Type == "Supervised")
write_csv(supervised_weights_export, paste0("SRC_", best_supervised, "_Supervised_Weights.csv"))

# Export unsupervised weights separately
unsupervised_weights_export <- all_weights_export %>%
  filter(Type == "Unsupervised", abs(as.numeric(Mean)) > 0.001)
write_csv(unsupervised_weights_export, paste0("SRC_", best_unsupervised, "_Unsupervised_Weights.csv"))

# Export SRC scores
src_scores_export <- src_scores %>%
  mutate(across(where(is.numeric), ~format_number(.x, 3)))

write_csv(src_scores_export, "SRC_Participant_Scores_All_Outcomes.csv")

# Export pairwise comparisons
cat("\nExporting pairwise comparisons...\n")
all_pairwise_export <- bind_rows(all_pairwise_comparisons)
all_pairwise_export <- all_pairwise_export %>%
  mutate(
    Weight_CI = paste0(
      format_number(Estimate, 3), " [",
      format_number(CI_Lower, 3), ", ",
      format_number(CI_Upper, 3), "]"
    ),
    Estimate_formatted = format_number(Estimate, 3),
    p_value_formatted = format_number(p_value, 4)
  ) %>%
  select(Outcome, Model, Comparison, Estimate = Estimate_formatted, 
         Weight_CI, p_value = p_value_formatted, Significance)

write_csv(all_pairwise_export, paste0("SRC_All_Pairwise_Comparisons_", best_supervised, ".csv"))

# Create summary of significant pairwise comparisons
sig_pairwise <- all_pairwise_export %>%
  filter(p_value < 0.05) %>%
  arrange(Outcome, Comparison)

if (nrow(sig_pairwise) > 0) {
  write_csv(sig_pairwise, paste0("SRC_Significant_Pairwise_Comparisons_", best_supervised, ".csv"))
  
  cat("\nSIGNIFICANT PAIRWISE COMPARISONS:\n")
  cat("---------------------------------\n")
  for (outcome in outcomes) {
    outcome_comparisons <- sig_pairwise %>% filter(Outcome == outcome)
    if (nrow(outcome_comparisons) > 0) {
      cat("\n", outcome, ":\n", sep = "")
      for (i in 1:nrow(outcome_comparisons)) {
        cat("  ", outcome_comparisons$Comparison[i], ": ", 
            outcome_comparisons$Weight_CI[i], outcome_comparisons$Significance[i], "\n", sep = "")
      }
    }
  }
}

# Create individual outcome weight tables
for (outcome in outcomes) {
  outcome_weights <- all_weights_export %>%
    filter(Outcome == outcome, Type == "Supervised") %>%
    mutate(Mean_numeric = as.numeric(Mean)) %>%
    arrange(desc(abs(Mean_numeric))) %>%
    select(-Mean_numeric)
  
  summary_table <- outcome_weights %>%
    select(Variable, Weight_CI, p_value, Significance, Direction) %>%
    mutate(
      Interpretation = case_when(
        Variable == "Maternal age ≤25" ~ "Being ≤25 years old",
        TRUE ~ "If present"
      )
    )
  
  write_csv(summary_table, paste0("SRC_", outcome, "_", best_supervised, "_weight_summary.csv"))
  
  # Add pairwise comparisons for this outcome
  outcome_pairwise <- all_pairwise_comparisons[[outcome]] %>%
    mutate(
      Weight_CI = paste0(
        format_number(Estimate, 3), " [",
        format_number(CI_Lower, 3), ", ",
        format_number(CI_Upper, 3), "]"
      ),
      p_value_formatted = format_number(p_value, 4)
    ) %>%
    select(Comparison, Weight_CI, p_value = p_value_formatted, Significance)
  
  write_csv(outcome_pairwise, paste0("SRC_", outcome, "_pairwise_comparisons.csv"))
}

# 14. Create summary tables ----
cat("\nCreating comprehensive summary tables for all outcomes...\n")

# Create supervised model summary table
supervised_summary <- all_weights_export %>%
  filter(Type == "Supervised") %>%
  select(Outcome, Variable, Weight_CI, p_value, Significance) %>%
  mutate(
    Outcome_clean = case_when(
      Outcome == "cog_comp_score" ~ "Cognitive",
      Outcome == "lang_comp_score" ~ "Language", 
      Outcome == "motor_comp_score" ~ "Motor"
    ),
    Variable_clean = gsub("_", " ", Variable),
    Variable_clean = gsub("Income ", "Income: ", Variable_clean),
    Variable_clean = gsub("Education ", "Education: ", Variable_clean),
    Variable_clean = gsub("Employment ", "Employment: ", Variable_clean),
    Variable_clean = gsub("Family ", "Family: ", Variable_clean)
  )

# Create wide format for supervised results
supervised_wide <- supervised_summary %>%
  filter(Outcome == "cog_comp_score") %>%
  select(Variable = Variable_clean,
         Cognitive = Weight_CI,
         Cognitive_p = p_value,
         Cognitive_sig = Significance)

# Add Language results
lang_data <- supervised_summary %>%
  filter(Outcome == "lang_comp_score") %>%
  arrange(Variable) %>%
  select(Language = Weight_CI, Language_p = p_value, Language_sig = Significance)

supervised_wide <- cbind(supervised_wide, lang_data)

# Add Motor results
motor_data <- supervised_summary %>%
  filter(Outcome == "motor_comp_score") %>%
  arrange(Variable) %>%
  select(Motor = Weight_CI, Motor_p = p_value, Motor_sig = Significance)

supervised_wide <- cbind(supervised_wide, motor_data)

# Write supervised summary table
write_csv(supervised_wide, paste0("SRC_Table_1_", best_supervised, "_Supervised_Weights_Summary.csv"))

# Create publication-ready formatted table
pub_supervised <- supervised_wide %>%
  mutate(
    Cognitive_formatted = paste0(Cognitive, Cognitive_sig),
    Language_formatted = paste0(Language, Language_sig),
    Motor_formatted = paste0(Motor, Motor_sig)
  ) %>%
  select(Variable,
         `Cognitive Score` = Cognitive_formatted,
         `Language Score` = Language_formatted,
         `Motor Score` = Motor_formatted)

write_csv(pub_supervised, paste0("SRC_Table_1_Publication_", best_supervised, "_Weights.csv"))

# Similar process for unsupervised results
unsupervised_summary <- all_weights_export %>%
  filter(Type == "Unsupervised", abs(as.numeric(Mean)) > 0.001) %>%
  select(Outcome, Variable, Weight_CI, p_value, Significance) %>%
  mutate(
    Outcome_clean = case_when(
      Outcome == "cog_comp_score" ~ "Cognitive",
      Outcome == "lang_comp_score" ~ "Language", 
      Outcome == "motor_comp_score" ~ "Motor"
    ),
    Variable_clean = gsub("_", " ", Variable),
    Variable_clean = gsub("Income ", "Income: ", Variable_clean),
    Variable_clean = gsub("Education ", "Education: ", Variable_clean),
    Variable_clean = gsub("Employment ", "Employment: ", Variable_clean),
    Variable_clean = gsub("Family ", "Family: ", Variable_clean)
  )

# Create wide format for unsupervised results (if any non-zero weights exist)
if (nrow(unsupervised_summary) > 0) {
  # Get unique variables across all outcomes
  unique_vars <- unique(unsupervised_summary$Variable_clean)
  
  unsupervised_wide <- data.frame(Variable = unique_vars)
  
  for (outcome_name in c("cog_comp_score", "lang_comp_score", "motor_comp_score")) {
    outcome_label <- case_when(
      outcome_name == "cog_comp_score" ~ "Cognitive",
      outcome_name == "lang_comp_score" ~ "Language",
      outcome_name == "motor_comp_score" ~ "Motor"
    )
    
    outcome_data <- unsupervised_summary %>%
      filter(Outcome == outcome_name) %>%
      select(Variable = Variable_clean, Weight_CI, p_value, Significance)
    
    # Create empty columns for this outcome
    unsupervised_wide[[outcome_label]] <- ""
    unsupervised_wide[[paste0(outcome_label, "_p")]] <- ""
    unsupervised_wide[[paste0(outcome_label, "_sig")]] <- ""
    
    # Fill in values where they exist
    for (i in 1:nrow(outcome_data)) {
      var_idx <- which(unsupervised_wide$Variable == outcome_data$Variable[i])
      if (length(var_idx) > 0) {
        unsupervised_wide[var_idx, outcome_label] <- outcome_data$Weight_CI[i]
        unsupervised_wide[var_idx, paste0(outcome_label, "_p")] <- outcome_data$p_value[i]
        unsupervised_wide[var_idx, paste0(outcome_label, "_sig")] <- outcome_data$Significance[i]
      }
    }
  }
  
  write_csv(unsupervised_wide, paste0("SRC_Table_2_", best_unsupervised, "_Unsupervised_Weights_Summary.csv"))
  
  # Create publication-ready unsupervised table
  pub_unsupervised <- unsupervised_wide %>%
    mutate(
      Cognitive_formatted = paste0(Cognitive, Cognitive_sig),
      Language_formatted = paste0(Language, Language_sig),
      Motor_formatted = paste0(Motor, Motor_sig)
    ) %>%
    select(Variable,
           `Cognitive Score` = Cognitive_formatted,
           `Language Score` = Language_formatted,
           `Motor Score` = Motor_formatted)
  
  write_csv(pub_unsupervised, paste0("SRC_Table_2_Publication_", best_unsupervised, "_Weights.csv"))
}




################################################################################
#  Clinical Applicability Analysis: NDI and Composite Scores
#  Continuation of SRC analysis
################################################################################

# Additional required library
if (!require("pROC")) install.packages("pROC")
library(pROC)

# 16. Define and Calculate NDI ----
cat("\n", rep("=", 80), "\n", sep = "")
cat("CLINICAL APPLICABILITY ANALYSIS: NEURODEVELOPMENTAL IMPAIRMENT\n")
cat(rep("=", 80), "\n\n", sep = "")

# Define NDI as any Bayley-III score <85
df_ndi <- src_scores %>%
  mutate(
    NDI = as.numeric(cog_comp_score < 85 | lang_comp_score < 85 | motor_comp_score < 85),
    # Also create domain-specific NDI variables for supplementary analysis
    NDI_cognitive = as.numeric(cog_comp_score < 85),
    NDI_language = as.numeric(lang_comp_score < 85),
    NDI_motor = as.numeric(motor_comp_score < 85)
  )

# Calculate NDI prevalence
n_total <- nrow(df_ndi)
n_ndi <- sum(df_ndi$NDI, na.rm = TRUE)
pct_ndi <- round(100 * n_ndi / n_total, 1)

cat("NDI PREVALENCE:\n")
cat("--------------\n")
cat("Total participants:", n_total, "\n")
cat("Participants with NDI:", n_ndi, "(", pct_ndi, "%)\n", sep = "")
cat("  - Cognitive impairment:", sum(df_ndi$NDI_cognitive, na.rm = TRUE), 
    "(", round(100 * sum(df_ndi$NDI_cognitive, na.rm = TRUE) / n_total, 1), "%)\n", sep = "")
cat("  - Language impairment:", sum(df_ndi$NDI_language, na.rm = TRUE),
    "(", round(100 * sum(df_ndi$NDI_language, na.rm = TRUE) / n_total, 1), "%)\n", sep = "")
cat("  - Motor impairment:", sum(df_ndi$NDI_motor, na.rm = TRUE),
    "(", round(100 * sum(df_ndi$NDI_motor, na.rm = TRUE) / n_total, 1), "%)\n\n", sep = "")

# 17. Calculate AUC with Bootstrap CIs ----
cat("DISCRIMINATIVE ABILITY ANALYSIS:\n")
cat("-------------------------------\n")

# Function to calculate AUC with bootstrap CI
calculate_auc_with_ci <- function(predictor, outcome, n_boot = 2000) {
  # Remove missing values
  complete_data <- data.frame(
    predictor = predictor,
    outcome = outcome
  ) %>%
    filter(complete.cases(.))
  
  # Calculate ROC and AUC
  roc_obj <- roc(outcome ~ predictor, data = complete_data, quiet = TRUE)
  
  # Bootstrap CI using pROC's built-in function
  ci_obj <- ci.auc(roc_obj, method = "bootstrap", boot.n = n_boot, 
                   conf.level = 0.95, progress = "none")
  
  # Calculate optimal cutpoint using Youden index
  coords_obj <- coords(roc_obj, "best", ret = c("threshold", "sensitivity", "specificity"),
                       best.method = "youden")
  
  return(list(
    auc = as.numeric(ci_obj[2]),
    ci_lower = as.numeric(ci_obj[1]),
    ci_upper = as.numeric(ci_obj[3]),
    roc = roc_obj,
    optimal_cutpoint = coords_obj$threshold,
    sensitivity = coords_obj$sensitivity,
    specificity = coords_obj$specificity
  ))
}

# Calculate AUC for supervised composite score (using cognitive outcome scores)
cat("\nCalculating AUC for supervised composite score...\n")
auc_supervised <- calculate_auc_with_ci(
  predictor = -df_ndi$cog_comp_score_supervised,  # Negative because higher risk = lower outcome
  outcome = df_ndi$NDI,
  n_boot = 2000
)

cat("Supervised", best_supervised, "composite score:\n")
cat("  AUC = ", sprintf("%.3f", auc_supervised$auc), 
    " (95% CI: ", sprintf("%.3f", auc_supervised$ci_lower), 
    "-", sprintf("%.3f", auc_supervised$ci_upper), ")\n", sep = "")
cat("  Optimal cutpoint:", sprintf("%.3f", auc_supervised$optimal_cutpoint), "\n")
cat("  Sensitivity:", sprintf("%.1f%%", 100 * auc_supervised$sensitivity), "\n")
cat("  Specificity:", sprintf("%.1f%%", 100 * auc_supervised$specificity), "\n")

# Calculate AUC for unsupervised composite score
cat("\nCalculating AUC for unsupervised composite score...\n")
auc_unsupervised <- calculate_auc_with_ci(
  predictor = -df_ndi$cog_comp_score_unsupervised,  # Negative because higher risk = lower outcome
  outcome = df_ndi$NDI,
  n_boot = 2000
)

cat("Unsupervised", best_unsupervised, "composite score:\n")
cat("  AUC = ", sprintf("%.3f", auc_unsupervised$auc), 
    " (95% CI: ", sprintf("%.3f", auc_unsupervised$ci_lower), 
    "-", sprintf("%.3f", auc_unsupervised$ci_upper), ")\n", sep = "")
cat("  Optimal cutpoint:", sprintf("%.3f", auc_unsupervised$optimal_cutpoint), "\n")
cat("  Sensitivity:", sprintf("%.1f%%", 100 * auc_unsupervised$sensitivity), "\n")
cat("  Specificity:", sprintf("%.1f%%", 100 * auc_unsupervised$specificity), "\n")

# Test for significant difference between AUCs
cat("\nTesting difference between AUCs...\n")
roc_test <- roc.test(auc_supervised$roc, auc_unsupervised$roc, method = "delong")
cat("  DeLong test p-value:", sprintf("%.4f", roc_test$p.value), "\n")

# 18. Calculate Odds Ratios for Risk Tertiles ----
cat("\n\nRISK TERTILE ANALYSIS:\n")
cat("---------------------\n")

# Function to calculate OR with CI
calculate_tertile_or <- function(data, tertile_var, outcome_var) {
  # Create dummy variables for tertiles
  data_model <- data %>%
    mutate(
      low_risk = as.numeric(get(tertile_var) == "Low Risk"),
      moderate_risk = as.numeric(get(tertile_var) == "Moderate Risk"),
      high_risk = as.numeric(get(tertile_var) == "High Risk")
    ) %>%
    filter(!is.na(get(outcome_var)), !is.na(get(tertile_var)))
  
  # Fit logistic regression with low risk as reference
  model <- glm(as.formula(paste(outcome_var, "~ moderate_risk + high_risk")), 
               data = data_model, family = binomial())
  
  # Extract results
  summary_model <- summary(model)
  coef_table <- summary_model$coefficients
  
  # Calculate ORs and CIs
  or_moderate <- exp(coef_table["moderate_risk", "Estimate"])
  or_high <- exp(coef_table["high_risk", "Estimate"])
  
  ci_moderate <- exp(confint(model)["moderate_risk", ])
  ci_high <- exp(confint(model)["high_risk", ])
  
  # Get sample sizes per tertile
  n_per_tertile <- table(data_model[[tertile_var]], data_model[[outcome_var]])
  
  return(list(
    or_moderate = or_moderate,
    ci_moderate = ci_moderate,
    p_moderate = coef_table["moderate_risk", "Pr(>|z|)"],
    or_high = or_high,
    ci_high = ci_high,
    p_high = coef_table["high_risk", "Pr(>|z|)"],
    n_table = n_per_tertile,
    model = model
  ))
}

# Calculate ORs for supervised tertiles
cat("\nSupervised composite score tertiles:\n")
or_supervised <- calculate_tertile_or(df_ndi, "cog_comp_score_sup_tertile", "NDI")

cat("\nNDI by supervised risk tertile:\n")
print(or_supervised$n_table)
cat("\nProportion with NDI by tertile:\n")
prop_table <- prop.table(or_supervised$n_table, margin = 1)
cat("  Low Risk:", sprintf("%.1f%%", 100 * prop_table["Low Risk", "1"]), "\n")
cat("  Moderate Risk:", sprintf("%.1f%%", 100 * prop_table["Moderate Risk", "1"]), "\n")
cat("  High Risk:", sprintf("%.1f%%", 100 * prop_table["High Risk", "1"]), "\n")

cat("\nOdds Ratios (reference = Low Risk):\n")
cat("  Moderate vs Low: OR =", sprintf("%.2f", or_supervised$or_moderate),
    "(95% CI:", sprintf("%.2f", or_supervised$ci_moderate[1]), "-",
    sprintf("%.2f", or_supervised$ci_moderate[2]), "), p =",
    sprintf("%.4f", or_supervised$p_moderate), "\n")
cat("  High vs Low: OR =", sprintf("%.2f", or_supervised$or_high),
    "(95% CI:", sprintf("%.2f", or_supervised$ci_high[1]), "-",
    sprintf("%.2f", or_supervised$ci_high[2]), "), p =",
    sprintf("%.4f", or_supervised$p_high), "\n")

# Calculate ORs for unsupervised tertiles
cat("\n\nUnsupervised composite score tertiles:\n")
or_unsupervised <- calculate_tertile_or(df_ndi, "cog_comp_score_unsup_tertile", "NDI")

cat("\nNDI by unsupervised risk tertile:\n")
print(or_unsupervised$n_table)
cat("\nProportion with NDI by tertile:\n")
prop_table_unsup <- prop.table(or_unsupervised$n_table, margin = 1)
cat("  Low Risk:", sprintf("%.1f%%", 100 * prop_table_unsup["Low Risk", "1"]), "\n")
cat("  Moderate Risk:", sprintf("%.1f%%", 100 * prop_table_unsup["Moderate Risk", "1"]), "\n")
cat("  High Risk:", sprintf("%.1f%%", 100 * prop_table_unsup["High Risk", "1"]), "\n")

cat("\nOdds Ratios (reference = Low Risk):\n")
cat("  Moderate vs Low: OR =", sprintf("%.2f", or_unsupervised$or_moderate),
    "(95% CI:", sprintf("%.2f", or_unsupervised$ci_moderate[1]), "-",
    sprintf("%.2f", or_unsupervised$ci_moderate[2]), "), p =",
    sprintf("%.4f", or_unsupervised$p_moderate), "\n")
cat("  High vs Low: OR =", sprintf("%.2f", or_unsupervised$or_high),
    "(95% CI:", sprintf("%.2f", or_unsupervised$ci_high[1]), "-",
    sprintf("%.2f", or_unsupervised$ci_high[2]), "), p =",
    sprintf("%.4f", or_unsupervised$p_high), "\n")

# 19. Create ROC curve visualization ----
cat("\n\nCreating ROC curve visualization...\n")

# Prepare data for plotting
roc_supervised_df <- data.frame(
  fpr = 1 - auc_supervised$roc$specificities,
  tpr = auc_supervised$roc$sensitivities,
  model = paste0("Supervised (", best_supervised, ")")
)

roc_unsupervised_df <- data.frame(
  fpr = 1 - auc_unsupervised$roc$specificities,
  tpr = auc_unsupervised$roc$sensitivities,
  model = paste0("Unsupervised (", best_unsupervised, ")")
)

roc_combined <- rbind(roc_supervised_df, roc_unsupervised_df)

# Create ROC plot
p_roc <- ggplot(roc_combined, aes(x = fpr, y = tpr, color = model)) +
  geom_line(size = 1.2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
  scale_x_continuous(breaks = seq(0, 1, 0.2), limits = c(0, 1)) +
  scale_y_continuous(breaks = seq(0, 1, 0.2), limits = c(0, 1)) +
  scale_color_manual(values = c("Supervised (ridge)" = "#2E86AB", 
                                "Supervised (elastic)" = "#2E86AB",
                                "Supervised (lasso)" = "#2E86AB",
                                "Supervised (ols)" = "#2E86AB",
                                "Supervised (rf)" = "#2E86AB",
                                "Unsupervised (FAMD_Dim1)" = "#A23B72",
                                "Unsupervised (MCA_Dim1)" = "#A23B72",
                                "Unsupervised (PAM_Cluster)" = "#A23B72")) +
  labs(
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Model",
    title = "ROC Curves for Prediction of Neurodevelopmental Impairment",
    subtitle = paste0("Supervised AUC = ", sprintf("%.3f", auc_supervised$auc),
                      " (", sprintf("%.3f", auc_supervised$ci_lower), "-",
                      sprintf("%.3f", auc_supervised$ci_upper), ")",
                      "  |  Unsupervised AUC = ", sprintf("%.3f", auc_unsupervised$auc),
                      " (", sprintf("%.3f", auc_unsupervised$ci_lower), "-",
                      sprintf("%.3f", auc_unsupervised$ci_upper), ")")
  ) +
  theme_classic(base_size = 14) +
  theme(
    legend.position = c(0.7, 0.2),
    legend.background = element_rect(fill = "white", color = "black"),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(size = 12)
  ) +
  coord_equal()

ggsave("NDI_ROC_curves.png", p_roc, width = 8, height = 8, dpi = 300)

# 20. Create tertile risk visualization ----
cat("Creating tertile risk visualization...\n")

# Prepare data for tertile plot
tertile_data <- rbind(
  data.frame(
    model = paste0("Supervised\n(", best_supervised, ")"),
    tertile = c("Low Risk", "Moderate Risk", "High Risk"),
    prop_ndi = as.numeric(prop.table(or_supervised$n_table, margin = 1)[, "1"]),
    n_total = rowSums(or_supervised$n_table),
    n_ndi = or_supervised$n_table[, "1"]
  ),
  data.frame(
    model = paste0("Unsupervised\n(", best_unsupervised, ")"),
    tertile = c("Low Risk", "Moderate Risk", "High Risk"),
    prop_ndi = as.numeric(prop.table(or_unsupervised$n_table, margin = 1)[, "1"]),
    n_total = rowSums(or_unsupervised$n_table),
    n_ndi = or_unsupervised$n_table[, "1"]
  )
)

tertile_data$tertile <- factor(tertile_data$tertile, 
                               levels = c("Low Risk", "Moderate Risk", "High Risk"))

# Create bar plot
p_tertiles <- ggplot(tertile_data, aes(x = tertile, y = prop_ndi * 100, fill = model)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  geom_text(aes(label = paste0(round(prop_ndi * 100, 1), "%\n(", n_ndi, "/", n_total, ")")),
            position = position_dodge(width = 0.9), vjust = -0.5, size = 3.5) +
  scale_fill_manual(values = c("#2E86AB", "#A23B72")) +
  scale_y_continuous(limits = c(0, max(tertile_data$prop_ndi * 100) * 1.2),
                     expand = c(0, 0)) +
  labs(
    x = "Risk Tertile",
    y = "Proportion with NDI (%)",
    fill = "Model",
    title = "Neurodevelopmental Impairment by Risk Tertile",
    subtitle = "Numbers show % (n with NDI / n total)"
  ) +
  theme_classic(base_size = 14) +
  theme(
    legend.position = "top",
    plot.title = element_text(face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14, face = "bold")
  )

ggsave("NDI_by_risk_tertiles.png", p_tertiles, width = 10, height = 8, dpi = 300)

# 21. Export NDI analysis results ----
cat("\n\nExporting NDI analysis results...\n")

# Create summary data frame
ndi_summary <- data.frame(
  Analysis = c(
    "Total participants",
    "Participants with NDI",
    "NDI prevalence (%)",
    "",
    "Supervised model",
    paste0("  AUC (95% CI)"),
    "  Optimal cutpoint",
    "  Sensitivity (%)",
    "  Specificity (%)",
    "",
    "Unsupervised model", 
    paste0("  AUC (95% CI)"),
    "  Optimal cutpoint",
    "  Sensitivity (%)",
    "  Specificity (%)",
    "",
    "DeLong test p-value",
    "",
    "Supervised tertiles",
    "  Low risk: NDI rate (%)",
    "  Moderate risk: NDI rate (%)",
    "  High risk: NDI rate (%)",
    "  OR Moderate vs Low (95% CI)",
    "  OR High vs Low (95% CI)",
    "",
    "Unsupervised tertiles",
    "  Low risk: NDI rate (%)",
    "  Moderate risk: NDI rate (%)", 
    "  High risk: NDI rate (%)",
    "  OR Moderate vs Low (95% CI)",
    "  OR High vs Low (95% CI)"
  ),
  Value = c(
    as.character(n_total),
    paste0(n_ndi, " (", pct_ndi, "%)"),
    as.character(pct_ndi),
    "",
    best_supervised,
    paste0(sprintf("%.3f", auc_supervised$auc), " (",
           sprintf("%.3f", auc_supervised$ci_lower), "-",
           sprintf("%.3f", auc_supervised$ci_upper), ")"),
    sprintf("%.3f", auc_supervised$optimal_cutpoint),
    sprintf("%.1f", 100 * auc_supervised$sensitivity),
    sprintf("%.1f", 100 * auc_supervised$specificity),
    "",
    best_unsupervised,
    paste0(sprintf("%.3f", auc_unsupervised$auc), " (",
           sprintf("%.3f", auc_unsupervised$ci_lower), "-",
           sprintf("%.3f", auc_unsupervised$ci_upper), ")"),
    sprintf("%.3f", auc_unsupervised$optimal_cutpoint),
    sprintf("%.1f", 100 * auc_unsupervised$sensitivity),
    sprintf("%.1f", 100 * auc_unsupervised$specificity),
    "",
    sprintf("%.4f", roc_test$p.value),
    "",
    "",
    sprintf("%.1f", 100 * prop_table["Low Risk", "1"]),
    sprintf("%.1f", 100 * prop_table["Moderate Risk", "1"]),
    sprintf("%.1f", 100 * prop_table["High Risk", "1"]),
    paste0(sprintf("%.2f", or_supervised$or_moderate), " (",
           sprintf("%.2f", or_supervised$ci_moderate[1]), "-",
           sprintf("%.2f", or_supervised$ci_moderate[2]), "), p=",
           sprintf("%.4f", or_supervised$p_moderate)),
    paste0(sprintf("%.2f", or_supervised$or_high), " (",
           sprintf("%.2f", or_supervised$ci_high[1]), "-",
           sprintf("%.2f", or_supervised$ci_high[2]), "), p=",
           sprintf("%.4f", or_supervised$p_high)),
    "",
    "",
    sprintf("%.1f", 100 * prop_table_unsup["Low Risk", "1"]),
    sprintf("%.1f", 100 * prop_table_unsup["Moderate Risk", "1"]),
    sprintf("%.1f", 100 * prop_table_unsup["High Risk", "1"]),
    paste0(sprintf("%.2f", or_unsupervised$or_moderate), " (",
           sprintf("%.2f", or_unsupervised$ci_moderate[1]), "-",
           sprintf("%.2f", or_unsupervised$ci_moderate[2]), "), p=",
           sprintf("%.4f", or_unsupervised$p_moderate)),
    paste0(sprintf("%.2f", or_unsupervised$or_high), " (",
           sprintf("%.2f", or_unsupervised$ci_high[1]), "-",
           sprintf("%.2f", or_unsupervised$ci_high[2]), "), p=",
           sprintf("%.4f", or_unsupervised$p_high))
  )
)

write_csv(ndi_summary, "NDI_Analysis_Summary.csv")

# Export detailed results
ndi_detailed <- df_ndi %>%
  select(study_id, NDI, NDI_cognitive, NDI_language, NDI_motor,
         cog_comp_score_supervised, cog_comp_score_unsupervised,
         cog_comp_score_sup_tertile, cog_comp_score_unsup_tertile,
         cog_comp_score, lang_comp_score, motor_comp_score)

write_csv(ndi_detailed, "NDI_Participant_Level_Data.csv")

# 22. Generate manuscript text ----
cat("\n", rep("=", 80), "\n", sep = "")
cat("MANUSCRIPT TEXT FOR NDI ANALYSIS:\n")
cat(rep("=", 80), "\n\n", sep = "")

cat("To further evaluate clinical applicability, we examined the association between\n")
cat("our composite scores and neurodevelopmental impairment (NDI), a clinically\n")
cat("relevant outcome commonly used in neonatal intervention trials. NDI was defined\n")
cat("as a score <85 on the Bayley-III cognitive, language, or motor composite score.\n")
cat("Among the", n_total, "participants,", n_ndi, "(", pct_ndi, "%) met criteria for NDI.\n\n", sep = "")

cat("The supervised", best_supervised, "composite score demonstrated", 
    ifelse(auc_supervised$auc >= 0.8, "excellent", 
           ifelse(auc_supervised$auc >= 0.7, "good", "moderate")),
    "discriminative ability with an AUC of", sprintf("%.3f", auc_supervised$auc),
    "(95% CI:", sprintf("%.3f", auc_supervised$ci_lower), "-",
    sprintf("%.3f", auc_supervised$ci_upper), "), as did the unsupervised",
    best_unsupervised, "approach with an AUC of", sprintf("%.3f", auc_unsupervised$auc),
    "(95% CI:", sprintf("%.3f", auc_unsupervised$ci_lower), "-",
    sprintf("%.3f", auc_unsupervised$ci_upper), ").\n\n")

cat("Infants in the highest risk tertile had", sprintf("%.2f", or_supervised$or_high),
    "times higher odds of NDI compared to those in the lowest risk tertile\n")
cat("(OR =", sprintf("%.2f", or_supervised$or_high), ", 95% CI:",
    sprintf("%.2f", or_supervised$ci_high[1]), "-",
    sprintf("%.2f", or_supervised$ci_high[2]), ", p =",
    ifelse(or_supervised$p_high < 0.001, "<0.001", sprintf("%.3f", or_supervised$p_high)),
    ") using the supervised score,\n", sep = "")
cat("with similar findings for the unsupervised approach (OR =",
    sprintf("%.2f", or_unsupervised$or_high), ", 95% CI:",
    sprintf("%.2f", or_unsupervised$ci_high[1]), "-",
    sprintf("%.2f", or_unsupervised$ci_high[2]), ", p =",
    ifelse(or_unsupervised$p_high < 0.001, "<0.001", sprintf("%.3f", or_unsupervised$p_high)),
    ").\n", sep = "")

cat("\n", rep("=", 80), "\n", sep = "")
cat("NDI ANALYSIS COMPLETE!\n")
cat(rep("=", 80), "\n\n", sep = "")

cat("New output files:\n")
cat("- NDI_Analysis_Summary.csv: Complete summary of NDI analysis results\n")
cat("- NDI_Participant_Level_Data.csv: Participant-level NDI and score data\n")
cat("- NDI_ROC_curves.png: ROC curves comparing model performance\n")
cat("- NDI_by_risk_tertiles.png: Bar chart of NDI rates by risk tertile\n")

# Save updated workspace
save.image("SRC_Analysis_Complete_with_NDI.RData")
